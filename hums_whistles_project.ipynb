{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvrRvdTwxnvV"
   },
   "source": [
    "# Environment set up\n",
    "\n",
    "\n",
    "Our first step is to load the Python libraries that we will be using in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9gjrwd7Cxez5",
    "outputId": "c054edf1-d6b2-4a59-c4b7-2fc1d12527fb"
   },
   "outputs": [],
   "source": [
    "import os, glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aD2rP-8AzWS4"
   },
   "source": [
    "# Exploring our dataset sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fddX9WV_0oWx"
   },
   "source": [
    "Run the following cell to check the contents of our project directory. The output should include this Jupyter notebook and the `data` directory containing our recordings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mroJNE2m0nI9",
    "outputId": "33f1e881-e5b6-4a0f-e454-512029c1630a"
   },
   "outputs": [],
   "source": [
    "os.listdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many audio files we have in the `data` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yOPTdwA9zeyz",
    "outputId": "4ddebd89-224a-4741-bb54-cfecdcab5fbc"
   },
   "outputs": [],
   "source": [
    "sample_path = 'data/*.wav'\n",
    "files = glob.glob(sample_path)\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0OeDvt7zedb"
   },
   "source": [
    "This figure should be interpreted as the number of **samples** in our dataset. Let's listen to some random audio files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "id": "tcqXgKzgzmwG",
    "outputId": "73246388-afb9-4897-af52-a8f1f056b044"
   },
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "  n = np.random.randint(98)\n",
    "  display(ipd.Audio(files[n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3qkV3Im2WET"
   },
   "source": [
    "Can you recognise each song? Can you tell whether the interpreters are humming or whistling to the song?\n",
    "\n",
    "Let's have a look at the name of the 98 audio file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LHa-FqEg2Gyu",
    "outputId": "b0cc4ea0-529d-4aeb-b72c-7be45cf1c382"
   },
   "outputs": [],
   "source": [
    "for file in files:\n",
    "  print(file.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lh3--lJb5AEk"
   },
   "source": [
    "The name of each file follows the naming convention `[Participant ID]_[type of recording]_[song].wav`. We can parse each file name and extract this information. Let's do it for the first one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zOg8q-yE4_Dr",
    "outputId": "b0b09e6a-b719-4257-cf8f-da24a85aebc5"
   },
   "outputs": [],
   "source": [
    "print('The full path to the first audio file is: ', files[0])\n",
    "print('\\n')\n",
    "print('The name of the first audio file is: ', files[0].split('/')[-1])\n",
    "print('    The participand ID is: ', files[0].split('/')[-1].split('_')[0])\n",
    "print('    The type of interpretation is: ', files[0].split('/')[-1].split('_')[1])\n",
    "print('    The song is: ', files[0].split('_')[2].split('.')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdTayCxW7t1z"
   },
   "source": [
    "Now that we know how to extract participant ID, type of interpretation and song, let's create a table-like structure using Python lists that collects all this information from each of the audio files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "10zSKNKQFwDc",
    "outputId": "4d09f4c2-c891-4300-d951-959db5ae357d"
   },
   "outputs": [],
   "source": [
    "MLENDHW_table = [] \n",
    "\n",
    "for file in files:\n",
    "  file_name = file.split('/')[-1]\n",
    "  participant_ID = file.split('/')[-1].split('_')[0]\n",
    "  interpretation_type = file.split('/')[-1].split('_')[1]\n",
    "  song = file.split('/')[-1].split('_')[2].split('.')[0]\n",
    "  MLENDHW_table.append([file_name,participant_ID,interpretation_type, song])\n",
    "\n",
    "MLENDHW_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9Okox2iI87b"
   },
   "source": [
    "Next we load this table into a Pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "3nhyvDeZGnGr",
    "outputId": "859a2244-f676-4773-e8df-6f0ca87c8604"
   },
   "outputs": [],
   "source": [
    "MLENDHW_df = pd.DataFrame(MLENDHW_table,columns=['file_id','participant','interpretation','song']).set_index('file_id') \n",
    "MLENDHW_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step, we have created a Pandas DataFrame representing our dataset. Our dataset consists of 98 samples described by 4 attributes, namely audio recording, participant ID, type of interpretation and name of the song. \n",
    "\n",
    "In this machine learning project we want to build a prediction pipeline that takes an audio recording as an input and labels it as either hum or whistle. We can use the audio recording and type of interpretation attributes of the samples in this dataset to build our target prediction pipeline.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJVADYvuHq3x"
   },
   "source": [
    "# Feature extraction\n",
    "\n",
    "Audio files are complex data types. Specifically they are **discrete signals** or **time series**, consisting of values on a 1D temporal grid. These values are known as *samples* in signal theory, which might be a bit confusing, as we have used the term *sample* to refer to the *items* in our dataset. \n",
    "\n",
    "The **sampling frequency** is the rate at which samples in an audio file are produced. For instance a sampling frequency of 5HZ indicates that we produce 5 samples per second, or 1 sample every 0.2 s.\n",
    "\n",
    "Let's plot one of our audio signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "cU7cSdySHx8P",
    "outputId": "2db6b0fb-b640-4529-b1e7-e306bc59dbd8"
   },
   "outputs": [],
   "source": [
    "n=0\n",
    "fs = None \n",
    "\n",
    "x, fs = librosa.load(files[n],sr=fs)\n",
    "t = np.arange(len(x))/fs\n",
    "\n",
    "plt.plot(t,x)\n",
    "plt.xlabel('time (sec)')\n",
    "plt.ylabel('amplitude')\n",
    "plt.show()\n",
    "\n",
    "display(ipd.Audio(files[n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVndEmbJIR5-"
   },
   "source": [
    "Can you recognise the song and interpretation type? Does it agree with the values shown in the ``` MLENDHW_df ``` dataframe? Let's check it:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P4cSPJxnT80j",
    "outputId": "dc7eb544-d165-4c1b-825c-ebff7d369c7d"
   },
   "outputs": [],
   "source": [
    "MLENDHW_df.loc[files[n].split('/')[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Weh65CY7T9da"
   },
   "source": [
    "By changing the value of `n`, you can listen to other examples.\n",
    "\n",
    "Exactly, how complex is an audio signal? Let's start by looking at the number of samples in our first audio files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5oSPjIW9JZWv",
    "outputId": "6a39e741-6c6e-4b95-de80-af6bb575f838"
   },
   "outputs": [],
   "source": [
    "n=0\n",
    "x, fs = librosa.load(files[n],sr=fs)\n",
    "\n",
    "print('This audio signal has', len(x), 'samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrNs96siJggo"
   },
   "source": [
    "If we are using a raw audio signal as the input of a machine learning model, we will be operating in a predictor space consisting of hundreds of thousands of dimensions. Compare this figure with the number of samples that we have in our dataset. Do you think we have enough samples to train a model that takes one of these audio signals as an input? \n",
    "\n",
    "One approach to deal with high dimensionality is to extract a few features from our signals and use these features as predictors, instead of the raw audio signal. In this notebook we will define four audio features, namely:\n",
    "\n",
    "\n",
    "1.   Power.\n",
    "2.   Fraction of voiced region.\n",
    "3.   Pitch mean.\n",
    "4.   Pitch standard deviation.\n",
    "   \n",
    "\n",
    "In the next cell, we define a new function that gets the pitch of an audio signal (do not worry if you do not know what it is, but feel free to independently read about it!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiXYO1tdJgIs"
   },
   "outputs": [],
   "source": [
    "def getPitch(x,fs,winLen=0.02):\n",
    "  p = winLen*fs\n",
    "  frame_length = int(2**int(p-1).bit_length())\n",
    "  hop_length = frame_length//2\n",
    "  f0, voiced_flag, voiced_probs = librosa.pyin(y=x, fmin=80, fmax=450, sr=fs,\n",
    "                                                 frame_length=frame_length,hop_length=hop_length)\n",
    "  return f0,voiced_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCRRi_NFM3kw"
   },
   "source": [
    "Then next cell defines a function that returns a NumPy array (`X`) containing the 2 of the audio features (namely 'power' and 'fraction of voiced region'), which will be used as predictors, and a binary Numpy array (`y`), that indicates whether the type of interpretation is a hum (`y=1`) or whistle (`y=0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NEMwPrVmMHZa"
   },
   "outputs": [],
   "source": [
    "def getXy(files,labels_file, scale_audio=False, onlySingleDigit=False):\n",
    "  X,y =[],[]\n",
    "  for file in tqdm(files):\n",
    "    fileID = file.split('/')[-1]\n",
    "    file_name = file.split('/')[-1]\n",
    "    yi = labels_file.loc[fileID]['interpretation']=='hum'\n",
    "\n",
    "    fs = None \n",
    "    x, fs = librosa.load(file,sr=fs)\n",
    "    if scale_audio: x = x/np.max(np.abs(x))\n",
    "    f0, voiced_flag = getPitch(x,fs,winLen=0.02)\n",
    "      \n",
    "    power = np.sum(x**2)/len(x)\n",
    "    voiced_fr = np.mean(voiced_flag)\n",
    "    pitch_mean = np.nanmean(f0) if np.mean(np.isnan(f0))<1 else 0\n",
    "    pitch_std  = np.nanstd(f0) if np.mean(np.isnan(f0))<1 else 0\n",
    "\n",
    "    xi = [power, voiced_fr]\n",
    "    X.append(xi)\n",
    "    y.append(yi)\n",
    "\n",
    "  return np.array(X),np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQdqy2ksNCD7"
   },
   "source": [
    "Let's apply `getXy` to the subsample and obtain the NumPy predictor array (`X`) and a binary label (`y`). This could take a while, as we are processing each of the 98 recordings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jkjX_Zu9NNUs",
    "outputId": "aa4229a4-01d5-46fe-efd4-e544136cff82"
   },
   "outputs": [],
   "source": [
    "X,y = getXy(files, labels_file=MLENDHW_df, scale_audio=True, onlySingleDigit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOA190Z0kSVq"
   },
   "source": [
    "The next cell shows the shape of `X` and `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FcDtFySakTeP",
    "outputId": "d376d806-332f-47e6-b8d0-daf37f0665f0"
   },
   "outputs": [],
   "source": [
    "print('The shape of X is', X.shape) \n",
    "print('The shape of y is', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVvbOCDBZbQx"
   },
   "source": [
    "As you can see, we have 98 items consisting of 2 features (stored in `X`) and one binary label (stored in `y`). Is our dataset balanced? Let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tW_grfnuYmjD",
    "outputId": "002223e7-2529-437a-b817-03dbe96ecc65"
   },
   "outputs": [],
   "source": [
    "print(' The number of hum recordings is ', np.count_nonzero(y))\n",
    "print(' The number of whistle recordings is ', y.size - np.count_nonzero(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kp1Kgp5OkdC-"
   },
   "source": [
    "# Modeling\n",
    "\n",
    "Let's train a machine learning model using the dataset that we have just created. We will first split the dataset defined by X and y into a training set and a validation set. Then we will apply the SVM method provided by scikit-learn to the training dataset and finally will compute the accuracy of the trained model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uv0rBpWFkoe6",
    "outputId": "8a7f8e34-5e69-48ff-df1a-305424f98dc2"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.3)\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqPeEFEqk_cS"
   },
   "source": [
    "Can you identify the number of items in the training and validation sets?\n",
    "\n",
    "Let's now fit an SVM model and print both the training accuracty and validation accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cZYHghqhlDeM",
    "outputId": "3a07bbf7-256c-4569-b7c6-98582aeebab2"
   },
   "outputs": [],
   "source": [
    "model  = svm.SVC(C=1)\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "yt_p = model.predict(X_train)\n",
    "yv_p = model.predict(X_val)\n",
    "\n",
    "print('Training Accuracy', np.mean(yt_p==y_train))\n",
    "print('Validation  Accuracy', np.mean(yv_p==y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFr71A-blKsN"
   },
   "source": [
    "Compare the training and validation accuracies. Is our model overfitting, underfitting, performing well? What do you think the accuracy of a random classifier would be?\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of MLEnd_2021_Starter_kit.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "MLEnd",
   "language": "python",
   "name": "mlend"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
